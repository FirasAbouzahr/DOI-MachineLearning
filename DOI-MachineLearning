{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda0f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import resources\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import umap.umap_ as umap\n",
    "from tqdm import tqdm\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5b718",
   "metadata": {},
   "source": [
    "## Technical functions specific to our detector setup\n",
    "- converts the readout electronic's pixel IDs to more intuitive, chonrological IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data of interest was collected in these detector channels\n",
    "DOIchannels = [72, 73, 80, 81, 88, 89, 96, 97]\n",
    "\n",
    "roughChannels = np.array([[ 78,  73],\n",
    "                 [ 79,  72],\n",
    "                 [ 86,  81],\n",
    "                 [ 87,  80],\n",
    "                 [ 94,  89],\n",
    "                 [ 95,  88],\n",
    "                 [102,  97],\n",
    "                 [103,  96]])\n",
    "\n",
    "#converts PETSys ID to geometric ID ; the d\n",
    "def toGeo(x):\n",
    "    y = 8*indices.get(x)[0] + indices.get(x)[1]\n",
    "    return y\n",
    "\n",
    "indices = {\n",
    "      0 : (4,7-7),\n",
    "      1 : (4,7-6),\n",
    "      2 : (7,7-5),\n",
    "      3 : (5,7-7),\n",
    "      4 : (5,7-4),\n",
    "      5 : (5,7-5),\n",
    "      6 : (4,7-4),\n",
    "      7 : (7,7-7),\n",
    "      8 : (6,7-6),\n",
    "      9 : (7,7-4),\n",
    "      10 : (5,7-6),\n",
    "      11 : (6,7-4),\n",
    "      12 : (4,7-5),\n",
    "      13 : (6,7-5),\n",
    "      14 : (6,7-7),\n",
    "      15 : (7,7-6),\n",
    "      16 : (3,7-7),\n",
    "      17 : (3,7-6),\n",
    "      18 : (2,7-7),\n",
    "      19 : (2,7-6),\n",
    "      20 : (0,7-7),\n",
    "      21 : (1,7-7),\n",
    "      22 : (0,7-6),\n",
    "      23 : (1,7-6),\n",
    "      24 : (3,7-5),\n",
    "      25 : (1,7-5),\n",
    "      26 : (2,7-5),\n",
    "      27 : (4,7-3),\n",
    "      28 : (0,7-5),\n",
    "      29 : (3,7-4),\n",
    "      30 : (0,7-4),\n",
    "      31 : (1,7-4),\n",
    "      32 : (2,7-4),\n",
    "      33 : (3,7-3),\n",
    "      34 : (2,7-3),\n",
    "      35 : (0,7-3),\n",
    "      36 : (1,7-3),\n",
    "      37 : (0,7-2),\n",
    "      38 : (5,7-3),\n",
    "      39 : (1,7-2),\n",
    "      40 : (2,7-2),\n",
    "      41 : (3,7-2),\n",
    "      42 : (1,7-1),\n",
    "      43 : (0,7-1),\n",
    "      44 : (0,7-0),\n",
    "      45 : (3,7-1),\n",
    "      46 : (1,7-0),\n",
    "      47 : (2,7-1),\n",
    "      48 : (3,7-0),\n",
    "      49 : (2,7-0),\n",
    "      50 : (6,7-2),\n",
    "      51 : (6,7-1),\n",
    "      52 : (7,7-1),\n",
    "      53 : (4,7-1),\n",
    "      54 : (5,7-1),\n",
    "      55 : (6,7-0),\n",
    "      56 : (7,7-0),\n",
    "      57 : (7,7-2),\n",
    "      58 : (7,7-3),\n",
    "      59 : (4,7-2),\n",
    "      60 : (5,7-0),\n",
    "      61 : (5,7-2),\n",
    "      62 : (6,7-3),\n",
    "      63 : (4,7-0),\n",
    "      64:(3+8,7),\n",
    "      65:(3+8,6),\n",
    "      66:(2+8,4),\n",
    "      67:(2+8,6),\n",
    "      68:(3+8,4),\n",
    "      69:(1+8,7),\n",
    "      70:(1+8,5),\n",
    "      71:(0+8,7),\n",
    "      72:(1+8,6),\n",
    "      73:(3+8,3),\n",
    "      74:(2+8,7),\n",
    "      75:(2+8,3),\n",
    "      76:(3+8,5),\n",
    "      77:(0+8,5),\n",
    "      78:(2+8,5),\n",
    "      79:(0+8,6),\n",
    "      80:(4+8,7),\n",
    "      81:(6+8,7),\n",
    "      82:(5+8,7),\n",
    "      83:(7+8,7),\n",
    "      84:(5+8,6),\n",
    "      85:(4+8,6),\n",
    "      86:(6+8,6),\n",
    "      87:(7+8,6),\n",
    "      88:(4+8,5),\n",
    "      89:(6+8,5),\n",
    "      90:(5+8,5),\n",
    "      91:(1+8,4),\n",
    "      92:(7+8,5),\n",
    "      93:(7+8,4),\n",
    "      94:(6+8,4),\n",
    "      95:(4+8,4),\n",
    "      96:(5+8,4),\n",
    "      97:(5+8,3),\n",
    "      98:(6+8,3),\n",
    "      99:(4+8,3),\n",
    "      100:(7+8,3),\n",
    "      101:(7+8,2),\n",
    "      102:(0+8,4),\n",
    "      103:(6+8,2),\n",
    "      104:(7+8,1),\n",
    "      105:(5+8,2),\n",
    "      106:(6+8,1),\n",
    "      107:(4+8,2),\n",
    "      108:(7+8,0),\n",
    "      109:(5+8,1),\n",
    "      110:(6+8,0),\n",
    "      111:(4+8,1),\n",
    "      112:(5+8,0),\n",
    "      113:(4+8,0),\n",
    "      114:(0+8,2),\n",
    "      115:(2+8,1),\n",
    "      116:(0+8,1),\n",
    "      117:(3+8,1),\n",
    "      118:(1+8,1),\n",
    "      119:(1+8,0),\n",
    "      120:(0+8,0),\n",
    "      121:(1+8,2),\n",
    "      122:(1+8,3),\n",
    "      123:(3+8,2),\n",
    "      124:(2+8,0),\n",
    "      125:(2+8,2),\n",
    "      126:(0+8,3),\n",
    "      127:(3+8,0)}\n",
    "\n",
    "geo_channels = []\n",
    "for i in range(128):\n",
    "    geo_channels.append([i,toGeo(i)])\n",
    "geo_channels = np.asarray(geo_channels)\n",
    "    \n",
    "def toGeoChannelID(x):\n",
    "    x = x % 128\n",
    "    y = 8 * indices.get(x)[0] + indices.get(x)[1]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48d03b",
   "metadata": {},
   "source": [
    "## Data Read-In Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba616477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = /path/to/datafile\n",
    "# DOI = truth DOI from experiment \n",
    "# toGeo = convert channelIDs into geometric IDs\n",
    "# train_and_test = (T/F split into training & testing ; training size ; testing size)\n",
    "def getDOIDataFrame(file,DOI,toGeo = True,train_and_test = (True,0,0)):\n",
    "    df = pd.read_csv(file, delimiter=\"\\t\", usecols=(2,3,4,12,13,14))\n",
    "    df.columns = [\"TimeL\", \"ChargeL\", \"ChannelIDL\", \"TimeR\", \"ChargeR\", \"ChannelIDR\"]\n",
    "    df[\"DOI\"] = np.array([DOI]*np.shape(df)[0])\n",
    "\n",
    "    if toGeo == True:\n",
    "        df['ChannelIDL'] = df['ChannelIDL'].apply(toGeoChannelID)\n",
    "        df['ChannelIDR'] = df['ChannelIDR'].apply(toGeoChannelID)\n",
    "    \n",
    "    # only select channels coupled to the rough crystals \n",
    "    df = df[(df[\"ChannelIDR\"] == DOIchannels[0])\n",
    "    | (df[\"ChannelIDR\"] == DOIchannels[1])\n",
    "    | (df[\"ChannelIDR\"] == DOIchannels[2])\n",
    "    | (df[\"ChannelIDR\"] == DOIchannels[3])\n",
    "    | (df[\"ChannelIDR\"] == DOIchannels[4])\n",
    "    | (df[\"ChannelIDR\"] == DOIchannels[5])\n",
    "    | (df[\"ChannelIDR\"] == DOIchannels[6])\n",
    "    | (df[\"ChannelIDR\"] == DOIchannels[7])\n",
    "    ]\n",
    "    \n",
    "    # reset index now that we've dropped the uninteresting rows of data\n",
    "    df.index = np.arange(0,np.shape(df)[0],1)\n",
    "    \n",
    "    \n",
    "    if train_and_test[0] == True:\n",
    "        dfTraining = df[df.index < train_and_test[1]]\n",
    "        dfTesting = df[(df.index > train_and_test[1]) & (df.index < train_and_test[2])]\n",
    "        return dfTraining,dfTesting\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "\n",
    "def getChannelPairs(df,threshold):\n",
    "    channelData = df[['ChannelIDL','ChannelIDR']]\n",
    "    uniqueChannelPairs,Occurences = np.unique(channelData.to_numpy(),axis = 0,return_counts = True)\n",
    "    mostActiveChannels = np.where(Occurences >= threshold)[0]\n",
    "    uniqueChannelPairs = uniqueChannelPairs[mostActiveChannels]\n",
    "    return uniqueChannelPairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb8081",
   "metadata": {},
   "source": [
    "## Math Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x,A,mu,sig):\n",
    "    return A * np.exp(-((x-mu)/sig)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd563a43",
   "metadata": {},
   "source": [
    "## Pre-processing and analysis tools for DOI Machine Learning Project\n",
    "- The cells below contains all analysis functions needed to complete pre-processing and analysis of DOI data.\n",
    "- The results from these analyses codes will be used as our statistics for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc6a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guessing the standard deviation helps isolate the photopeak before we fit to it\n",
    "def getEnergySpectrum(df,channelID,side,bins,std_guess = 2,photopeakcut = 1.5,display = False):\n",
    "    fig,ax = plt.subplots()\n",
    "       \n",
    "    if side == 'left':\n",
    "        df_by_chan = df[df.ChannelIDL == channelID]\n",
    "        energy = df_by_chan.ChargeL\n",
    "    \n",
    "    else:\n",
    "        df_by_chan = df[df.ChannelIDR == channelID]\n",
    "        energy = df_by_chan.ChargeR\n",
    "    \n",
    "    y,x = np.histogram(energy,bins[0],(bins[1],bins[2]))\n",
    "    centers = (x[:-1] + x[1:]) / 2\n",
    "    \n",
    "    # this helps isolate the photopeak making it easier for the fitter to find it\n",
    "    fitcut = centers[np.where(y == max(y))[0][0]] - std_guess\n",
    "    \n",
    "    energy_temp = energy[energy >= fitcut] \n",
    "    \n",
    "    # redefine as we will fit to this cut data\n",
    "    y,x = np.histogram(energy_temp,bins[0],(bins[1],bins[2]))\n",
    "    centers = (x[:-1] + x[1:]) / 2\n",
    "    \n",
    "\n",
    "    guess = [max(y),centers[np.where(y == max(y))[0][0]],np.std(energy_temp)]\n",
    "        \n",
    "    try:\n",
    "        p,c = curve_fit(gaussian,centers,y,p0=guess)\n",
    "        xspace = np.linspace(p[1]-2.5*p[2],p[1]+2.5*p[2],500)\n",
    "        ax.plot(xspace,gaussian(xspace,*p),color = 'red')\n",
    "        ax.hist(energy,bins = np.linspace(bins[1],bins[2],bins[0]),color = 'C0')\n",
    "        photopeak_counts = energy[(energy >= p[1] - photopeakcut*p[2]) & (energy <= p[1] + photopeakcut*p[2])]\n",
    "    \n",
    "    except:\n",
    "        p = [-1,-1,-1]\n",
    "        photopeak_counts = np.array([])\n",
    "        print('Fit Failed')\n",
    "        \n",
    "    if display == False:\n",
    "        plt.close()\n",
    "        \n",
    "    return p,photopeak_counts\n",
    "\n",
    "\n",
    "# def plotEnergySpectrum()\n",
    "\n",
    "# normalized count differences is a common metric used for DOI identification\n",
    "def getNCD(left_Signal,right_signal):\n",
    "    NCD = (left_Signal - right_signal)/((left_Signal + right_signal))\n",
    "    return NCD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82023c39",
   "metadata": {},
   "source": [
    "## Umap projection function\n",
    "- must include DOI at the end of labels list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get umap projection of the data\n",
    "\n",
    "# df = (dataframe)\n",
    "# labels = (names of columns we want to use)\n",
    "# transform_to_zscore = (transforming to z-score can help the clustering algorithim)\n",
    "def umapProjection(df,labels,transform_to_zscore = False):\n",
    "    classes = [\"2 mm\",\"5 mm\",\"10 mm\",\"15 mm\",\"20 mm\",\"25 mm\",\"28 mm\"]\n",
    "    colors = ['C0','C1','C2','C3','C4','C5','C6']\n",
    "    # start umap instance and compute the dimensionality reduction\n",
    "    reducer = umap.UMAP()\n",
    "    umapLabels = labels\n",
    "    umapData = df[umapLabels].values\n",
    "\n",
    "    # sometimes helps with the clustering algorithim\n",
    "    if transform_to_zscore == True:\n",
    "        umapData = StandardScaler().fit_transform(umapData)\n",
    "\n",
    "    umapData = reducer.fit_transform(umapData)\n",
    "    \n",
    "    # plot the projection\n",
    "    fig,ax = plt.subplots(figsize = (10,7))\n",
    "    scatter = ax.scatter(\n",
    "        umapData[:, 0],\n",
    "        umapData[:, 1],c = [sns.color_palette()[x] for x in df.DOI.map({2:0, 5:1, 10:2, 15:3 , 20:4 , 25:5, 28:6})])\n",
    "\n",
    "    recs = []\n",
    "\n",
    "    for i in range(0,len(colors)):\n",
    "        recs.append(mpatches.Rectangle((0,0),1,1,fc=colors[i]))\n",
    "    ax.legend(recs,classes,loc = 'best',fontsize = 13)\n",
    "\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.title('UMAP projection of DOI', fontsize=24);\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599cedc",
   "metadata": {},
   "source": [
    "## defining our training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ebfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our raw training and testing datasets, we'll start small for now\n",
    "# all channels analysis\n",
    "\n",
    "dir = 'DOI_Data/'\n",
    "DOIs = [2,5,10,15,20,25,28]\n",
    "\n",
    "\n",
    "trainingDataList = []\n",
    "testingDataList = []\n",
    "\n",
    "for depth in DOIs:\n",
    "    training,testing = getDOIDataFrame('DOI_Data/28um_DOI_' + str(depth) + 'mm_coinc.txt',DOI=depth,train_and_test = (True,50000,70000))\n",
    "    \n",
    "    # shuffle the dataframe so our depths are random and not in order\n",
    "    training = training.sample(frac = 1)\n",
    "    testing = testing.sample(frac = 1)\n",
    "    \n",
    "    # only use populated channel pairs, less populated channel pairs are indictaive of connection via scattering.\n",
    "    trainingPairs =  roughChannels\n",
    "    testingPairs = trainingPairs #getChannelPairs(testing,350)\n",
    "    \n",
    "    for channelpairTrain,channelpairTest in zip(trainingPairs,testingPairs):\n",
    "        \n",
    "        training2 = training[(training.ChannelIDL == channelpairTrain[0]) & (training.ChannelIDR == channelpairTrain[1])]\n",
    "        testing2 = testing[(testing.ChannelIDL == channelpairTest[0]) & (testing.ChannelIDR == channelpairTest[1])]\n",
    "        \n",
    "        trainingDataList.append(training2)\n",
    "        testingDataList.append(testing2)\n",
    "    \n",
    "# convert back into dataframes & reset the index\n",
    "trainingData = pd.concat(trainingDataList,ignore_index=True)\n",
    "testingData = pd.concat(testingDataList,ignore_index=True)\n",
    "\n",
    "print(np.shape(trainingData),np.shape(testingData))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b21aab",
   "metadata": {},
   "source": [
    "### Fit to energy and then impose the photopeak energy cut on our datafranes\n",
    "- a bit of tedious code because we need to do this for both left & right detectors and both training & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_energy_cut = True\n",
    "\n",
    "energy_bins = (100,0,40)\n",
    "energyCut = 2 # number of sigma away from photopeak mean we want to keep \n",
    "\n",
    "\n",
    "# for each channel pair, histogram its energy and fit to it then remove channels that don't fall into the photopeak\n",
    "if run_energy_cut == True:\n",
    "\n",
    "    energyCutDict = {'ChannelID':[],'energyCut':[],'DOI':[]}\n",
    "    for chanL in tqdm(np.unique(trainingData.ChannelIDL)):\n",
    "        for depth in DOIs:\n",
    "            tempdf = trainingData[trainingData.DOI == depth]\n",
    "            p,_ = getEnergySpectrum(tempdf,chanL,'left',bins=energy_bins)\n",
    "            energyCutDict['ChannelID'] += [chanL]\n",
    "            energyCutDict['energyCut'] += [[p[1]-energyCut*p[2],p[1]+energyCut*p[2]]]\n",
    "            energyCutDict['DOI'] += [depth]\n",
    "            \n",
    "    for chanR in tqdm(np.unique(trainingData.ChannelIDR)):\n",
    "        for depth in DOIs:\n",
    "            tempdf = trainingData[trainingData.DOI == depth]\n",
    "            p,_ = getEnergySpectrum(tempdf,chanR,'right',bins=energy_bins)\n",
    "            energyCutDict['ChannelID'] += [chanR]\n",
    "            energyCutDict['energyCut'] += [[p[1]-energyCut*p[2],p[1]+energyCut*p[2]]]\n",
    "            energyCutDict['DOI'] += [depth]\n",
    "        \n",
    "    energyCutDf = pd.DataFrame(energyCutDict)\n",
    "\n",
    "    trainingArray = trainingData.to_numpy()\n",
    "    trainingDictEnergyCut = {\"TimeL\":[], \"ChargeL\":[], \"ChannelIDL\":[], \"TimeR\":[], \"ChargeR\":[], \"ChannelIDR\":[],'DOI':[]}\n",
    "    \n",
    "    for rownum in tqdm(range(len(trainingArray))):\n",
    "        row = trainingArray[rownum]\n",
    "        \n",
    "        # get the energy cut for each channel at the given DOI \n",
    "        ch1_cut = energyCutDf[(energyCutDf.ChannelID == row[2]) & (energyCutDf.DOI == row[6])].energyCut.iloc[0]\n",
    "        ch1_lower_limit,ch1_upper_limit = ch1_cut[0],ch1_cut[1]                        \n",
    "        ch2_cut = energyCutDf[(energyCutDf.ChannelID == row[5]) & (energyCutDf.DOI == row[6])].energyCut.iloc[0]\n",
    "        ch2_lower_limit,ch2_upper_limit = ch2_cut[0],ch2_cut[1]\n",
    "        \n",
    "        energy1,energy2 = row[1],row[4]\n",
    "        \n",
    "        \n",
    "        if energy1>=ch1_lower_limit and energy1 <=ch1_upper_limit and energy2>=ch2_lower_limit and energy2 <=ch2_upper_limit:\n",
    "            trainingDictEnergyCut[\"TimeL\"] += [row[0]]\n",
    "            trainingDictEnergyCut[\"ChargeL\"] += [row[1]]\n",
    "            trainingDictEnergyCut[\"ChannelIDL\"] += [row[2]]\n",
    "            trainingDictEnergyCut[\"TimeR\"] += [row[3]]\n",
    "            trainingDictEnergyCut[\"ChargeR\"] += [row[4]]\n",
    "            trainingDictEnergyCut[\"ChannelIDR\"] += [row[5]]\n",
    "        \n",
    "            trainingDictEnergyCut[\"DOI\"] += [row[6]]\n",
    "        \n",
    "    trainingData = pd.DataFrame(trainingDictEnergyCut) # redefine training data with energy cut\n",
    "\n",
    "\n",
    "# ###############################################################\n",
    "# ###############################################################\n",
    "# ########################### Testing Data ######################\n",
    "# ###############################################################\n",
    "# ###############################################################\n",
    "\n",
    "\n",
    "    energyCutDict = {'ChannelID':[],'energyCut':[],'DOI':[]}\n",
    "    for chanL in tqdm(np.unique(testingData.ChannelIDL)):\n",
    "        for depth in DOIs:\n",
    "            tempdf = testingData[testingData.DOI == depth]\n",
    "            p,_ = getEnergySpectrum(tempdf,chanL,'left',bins=energy_bins)\n",
    "            energyCutDict['ChannelID'] += [chanL]\n",
    "            energyCutDict['energyCut'] += [[p[1]-energyCut*p[2],p[1]+energyCut*p[2]]]\n",
    "            energyCutDict['DOI'] += [depth]\n",
    "            \n",
    "    for chanR in tqdm(np.unique(testingData.ChannelIDR)):\n",
    "        for depth in DOIs:\n",
    "            tempdf = testingData[testingData.DOI == depth]\n",
    "            p,_ = getEnergySpectrum(tempdf,chanR,'right',bins=energy_bins)\n",
    "            energyCutDict['ChannelID'] += [chanR]\n",
    "            energyCutDict['energyCut'] += [[p[1]-energyCut*p[2],p[1]+energyCut*p[2]]]\n",
    "            energyCutDict['DOI'] += [depth]\n",
    "        \n",
    "    energyCutDf = pd.DataFrame(energyCutDict)\n",
    "\n",
    "    testingArray = testingData.to_numpy()\n",
    "    testingDictEnergyCut = {\"TimeL\":[], \"ChargeL\":[], \"ChannelIDL\":[], \"TimeR\":[], \"ChargeR\":[], \"ChannelIDR\":[],'DOI':[]}\n",
    "    \n",
    "    for rownum in tqdm(range(len(testingArray))):\n",
    "        row = testingArray[rownum]\n",
    "        \n",
    "        ch1_cut = energyCutDf[(energyCutDf.ChannelID == row[2]) & (energyCutDf.DOI == row[6])].energyCut.iloc[0]\n",
    "        ch1_lower_limit,ch1_upper_limit = ch1_cut[0],ch1_cut[1]                        \n",
    "        ch2_cut = energyCutDf[(energyCutDf.ChannelID == row[5]) & (energyCutDf.DOI == row[6])].energyCut.iloc[0]\n",
    "        ch2_lower_limit,ch2_upper_limit = ch2_cut[0],ch2_cut[1]\n",
    "        \n",
    "        energy1,energy2 = row[1],row[4]\n",
    "        \n",
    "        if energy1>=ch1_lower_limit and energy1 <=ch1_upper_limit and energy2>=ch2_lower_limit and energy2 <=ch2_upper_limit:\n",
    "            testingDictEnergyCut[\"TimeL\"] += [row[0]]\n",
    "            testingDictEnergyCut[\"ChargeL\"] += [row[1]]\n",
    "            testingDictEnergyCut[\"ChannelIDL\"] += [row[2]]\n",
    "            \n",
    "            testingDictEnergyCut[\"TimeR\"] += [row[3]]\n",
    "            testingDictEnergyCut[\"ChargeR\"] += [row[4]]\n",
    "            testingDictEnergyCut[\"ChannelIDR\"] += [row[5]]\n",
    "        \n",
    "            testingDictEnergyCut[\"DOI\"] += [row[6]]\n",
    "        \n",
    "    testingData = pd.DataFrame(testingDictEnergyCut)\n",
    "\n",
    "print(np.shape(trainingData),np.shape(testingData))\n",
    "# statistics.append(np.shape(trainingData)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50761478",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData['NCD'] = ((trainingData.ChargeL - trainingData.ChargeR)/(trainingData.ChargeR + trainingData.ChargeL))\n",
    "trainingData['delta_t'] = (trainingData.TimeL - trainingData.TimeR)\n",
    "trainingData['delta_t'] = trainingData['delta_t'] - np.mean(trainingData['delta_t'])\n",
    "testingData['NCD'] = ((testingData.ChargeL - testingData.ChargeR)/(testingData.ChargeR + testingData.ChargeL))\n",
    "testingData['delta_t'] = (testingData.TimeL - testingData.TimeR)\n",
    "testingData['delta_t'] = testingData['delta_t'] - np.mean(testingData['delta_t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06575b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['NCD','ChargeR','ChargeL',\"ChannelIDL\",\"ChannelIDR\"]\n",
    "y = ['DOI']\n",
    "\n",
    "xTrain = trainingData[x]\n",
    "yTrain = trainingData[y]\n",
    "xTest = testingData[x]\n",
    "yTest = testingData[y]\n",
    "\n",
    "staticReduction = umap.UMAP(output_metric = \"chebyshev\",n_components=3,random_state=42,verbose=True).fit(xTrain)\n",
    "\n",
    "\n",
    "# plot the projection to see what its doing\n",
    "fig = plt.figure(figsize = (9,9))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "n1 = staticReduction.embedding_[:,0]\n",
    "n2 = staticReduction.embedding_[:,1]\n",
    "n3 = staticReduction.embedding_[:,2]\n",
    "# n4 = staticReduction.embedding_[:,3]\n",
    "DOIdensity = trainingData.DOI.to_numpy()\n",
    "\n",
    "scatter = plt.scatter(n1,n2,n3,c=DOIdensity,cmap='Spectral')\n",
    "\n",
    "bounds = [2,5,10,15,20,25,28,30] #DOIs\n",
    "fakebounds = [2-1,5-2.5,10-2,15-2,20-2,25-2,28-1,30-1]\n",
    "\n",
    "cbar = plt.colorbar(scatter,spacing='proportional', ticks=bounds, boundaries=np.array(fakebounds), format='%1i',label = 'DOI',fraction=0.03)\n",
    "ax.tick_params(\"z\",labelsize = 13)\n",
    "ax.tick_params(\"y\",labelsize = 13)\n",
    "ax.tick_params(\"x\",labelsize = 13)\n",
    "\n",
    "\n",
    "testEmbeddings = staticReduction.transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData['n1'] = n1\n",
    "trainingData['n2'] = n2\n",
    "trainingData['n3'] = n3\n",
    "# trainingData['n4'] = n4\n",
    "testingData['n1'] = testEmbeddings[:,0]\n",
    "testingData['n2'] = testEmbeddings[:,1]\n",
    "testingData['n3'] = testEmbeddings[:,2]\n",
    "# testingData['n4'] = testEmbeddings[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f1125",
   "metadata": {},
   "source": [
    "### this is a nice processing trick. Transform charge to z-score but do it per channel's charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cb582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def zscore_by_channel(df,col_name):\n",
    "    zscoredData = []\n",
    "    for chans in roughChannels:\n",
    "        tempdf = df[(df.ChannelIDL == chans[0]) & (df.ChannelIDR == chans[1])]\n",
    "        zscoredData.extend(stats.zscore(tempdf[col_name]))\n",
    "    return zscoredData\n",
    "\n",
    "rightNorms = []\n",
    "leftNorms = []\n",
    "for p in trainingPairs:\n",
    "    tempdf = trainingData[(trainingData.ChannelIDL == p[0]) & (trainingData.ChannelIDR == p[1])]\n",
    "    rightNorms.extend(stats.zscore(tempdf['ChargeR']))\n",
    "    leftNorms.extend(stats.zscore(tempdf['ChargeL']))\n",
    "    \n",
    "trainingData['ChargeL_zscore'] = np.array(leftNorms)\n",
    "trainingData['ChargeR_zscore'] = np.array(rightNorms)\n",
    "\n",
    "rightNorms = []\n",
    "leftNorms = []\n",
    "for p in trainingPairs:\n",
    "    tempdf = testingData[(testingData.ChannelIDL == p[0]) & (testingData.ChannelIDR == p[1])]\n",
    "    rightNorms.extend(stats.zscore(tempdf['ChargeR']))\n",
    "    leftNorms.extend(stats.zscore(tempdf['ChargeL']))\n",
    "    \n",
    "testingData['ChargeL_zscore'] = np.array(leftNorms)\n",
    "testingData['ChargeR_zscore'] = np.array(rightNorms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa614c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ChannelIDL','ChannelIDR','NCD','delta_t','ChargeR','ChargeL',\"n1\",\"n2\",\"n3\",\"ChargeR_zscore\",\"ChargeL_zscore\",'DOI']\n",
    "\n",
    "kerasTrainingFrame = tfdf.keras.pd_dataframe_to_tf_dataset(trainingData[features], label=\"DOI\")\n",
    "kerasPredictionFrame = tfdf.keras.pd_dataframe_to_tf_dataset(testingData[features], label=\"DOI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfdf.keras.RandomForestModel(verbose=2,max_depth = 10)\n",
    "model.fit(kerasTrainingFrame,label='DOI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "compilation = model.compile(metrics=[\"Accuracy\"])\n",
    "evaluation = model.evaluate(kerasPredictionFrame,return_dict=True)\n",
    "# accuracy.append(evaluation[\"Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "\n",
    "# plot inverse mean minimum \n",
    "importances = (model.make_inspector().variable_importances()['INV_MEAN_MIN_DEPTH'])\n",
    "used_features = []\n",
    "inv_mean_min_depth = []\n",
    "for feats in importances:\n",
    "    used_features.append(feats[0][0])\n",
    "    inv_mean_min_depth.append(feats[1])\n",
    "    \n",
    "mean_min_depth = 1/np.array(inv_mean_min_depth)\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (16,7))\n",
    "plt.bar(used_features,inv_mean_min_depth)\n",
    "plt.title('Inverse Mean Minumum Depth per Feature',fontsize = 18)\n",
    "plt.ylabel('Inverse Mean Minimum Depth',fontsize = 18)\n",
    "plt.xlabel('Feature',fontsize = 18)\n",
    "plt.xticks(fontsize = 15,rotation = 45)\n",
    "plt.yticks(fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's actually test our model\n",
    "predict = model.predict(kerasPredictionFrame)\n",
    "predict = np.argmax(predict, axis=1)\n",
    "predict = predict.reshape(predict.shape[0], 1)\n",
    "\n",
    "for name, value in evaluation.items():\n",
    "    print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d26982",
   "metadata": {},
   "source": [
    "### print accuracy per category & plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66912ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking accuracy per category!\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "normalize_per_category = True\n",
    "\n",
    "# define our confusion matrix\n",
    "matrix = np.transpose(confusion_matrix(predict,testingData.DOI))\n",
    "\n",
    "\n",
    "if normalize_per_category == True:\n",
    "    matrix = np.round(normalize(matrix,axis=1,norm='l1'),2)\n",
    "    lowcolor = 0.7\n",
    "else:\n",
    "    lowcolor = 4000\n",
    "\n",
    "DOI_index = 0\n",
    "correct = 0\n",
    "notCorrect = 0\n",
    "for row in matrix:\n",
    "    accuracy = row[DOI_index]/np.sum(row)\n",
    "    correct += row[DOI_index]\n",
    "    notCorrect += np.sum(row) - row[DOI_index]\n",
    "    print('Accuracy for ' + str(DOIs[DOI_index]) + ' mm DOI:',np.round(accuracy,3))\n",
    "    DOI_index += 1\n",
    "print('Total Accuracy: ',np.round(correct/(correct + notCorrect),3))\n",
    "\n",
    "# imshow will plot this by index in the list\n",
    "doiIndices = np.arange(0,7,1)\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (10,7))\n",
    "plt.imshow(matrix,cmap='Blues') \n",
    "plt.colorbar().set_label(label='Accuracy per Category',size=15)\n",
    "plt.xticks(doiIndices,DOIs,fontsize = 15)\n",
    "plt.yticks(doiIndices,DOIs,fontsize = 15)\n",
    "\n",
    "for i in doiIndices:\n",
    "    for j in doiIndices:\n",
    "        if matrix[j,i] < lowcolor:\n",
    "            plt.text(i,j,matrix[j,i],ha=\"center\", va=\"center\",c='black',fontsize = 13)\n",
    "        else:\n",
    "            plt.text(i,j,matrix[j,i],ha=\"center\", va=\"center\",c='w',fontsize = 13)\n",
    "\n",
    "plt.ylabel('Truth DOI',fontsize = 18)\n",
    "plt.xlabel('Predicted DOI',fontsize = 18)\n",
    "plt.title('Confusion Matrix with a 2$\\sigma$ cut',fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb7ec9",
   "metadata": {},
   "source": [
    "## Regression Models for predicting continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef856e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"NCD\",\"ChargeR\",\"ChargeL\",\"ChargeR_zscore\",\"ChargeL_zscore\",\"delta_t\",\"ChannelIDL\",\"ChannelIDR\",'DOI']\n",
    "kerasTrainingFrame_Regression = tfdf.keras.pd_dataframe_to_tf_dataset(trainingData[features], label=\"DOI\",task=tfdf.keras.Task.REGRESSION)\n",
    "kerasTestingFrame_Regression = tfdf.keras.pd_dataframe_to_tf_dataset(testingData[features], label=\"DOI\",task=tfdf.keras.Task.REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regression = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION,max_depth = 20)\n",
    "model_regression.fit(kerasTrainingFrame_Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regression.compile(metrics=[\"mse\"])\n",
    "evaluation = model_regression.evaluate(kerasTestingFrame_Regression, return_dict=True)\n",
    "\n",
    "print(evaluation)\n",
    "print()\n",
    "print(f\"MSE: {evaluation['mse']}\")\n",
    "print(f\"RMSE: {np.sqrt(evaluation['mse'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b11180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inverse mean minimum \n",
    "importances = (model_regression.make_inspector().variable_importances()['INV_MEAN_MIN_DEPTH'])\n",
    "used_features = []\n",
    "inv_mean_min_depth = []\n",
    "for feats in importances:\n",
    "    used_features.append(feats[0][0])\n",
    "    inv_mean_min_depth.append(feats[1])\n",
    "    \n",
    "\n",
    "fig,ax = plt.subplots(figsize = (16,7))\n",
    "plt.bar(used_features,inv_mean_min_depth)\n",
    "plt.title('Inverse Mean Minumum Depth per Feature',fontsize = 18)\n",
    "plt.ylabel('Inverse Mean Minimum Depth',fontsize = 18)\n",
    "plt.xlabel('Feature',fontsize = 18)\n",
    "plt.xticks(fontsize = 15,rotation = 45)\n",
    "plt.yticks(fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_regression.predict(kerasTestingFrame_Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bb213",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (10,7))\n",
    "newDOIs = DOIs[-1:]\n",
    "for i in DOIs:\n",
    "    whereDOI = np.where(testingData.DOI == i)[0]\n",
    "    bins = np.linspace(i-10,i+10,200)\n",
    "    y,x,_ = plt.hist(prediction[whereDOI],bins = bins,label = str(i) + ' mm')\n",
    "plt.ylabel('Counts',fontsize = 18)\n",
    "plt.xlabel('Predicted DOI [mm]',fontsize = 18)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('RF Regression DOI Prediction Distributions',fontsize = 18)\n",
    "leg = plt.legend(title = \"DOI\",fontsize = 13)\n",
    "leg.set_title(\"DOI\", prop = {'size':'x-large'})\n",
    "plt.xlim(0,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65834bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultFrame = pd.DataFrame(columns = [\"ChannelIDL\",\"ChannelIDR\",\"Truth\",\"Predicted\"])\n",
    "resultFrame[\"ChannelIDL\"] = testingData.ChannelIDL\n",
    "resultFrame[\"ChannelIDR\"] = testingData.ChannelIDR\n",
    "resultFrame[\"Truth\"] = testingData.DOI\n",
    "resultFrame[\"Predicted\"] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.shape(trainingData)[0]\n",
    "\n",
    "omit_2_28 = True\n",
    "print_averages = True\n",
    "\n",
    "\n",
    "if omit_2_28 == True: # necessary due to the asymmetric distributions\n",
    "    useDOIs = DOIs[1:6]\n",
    "    \n",
    "else:\n",
    "    useDOIs = DOIs\n",
    "    \n",
    "resolutionFrame = pd.DataFrame(columns = [\"ChannelIDL\",\"ChannelIDR\",\"DOI\",\"FWHM\",\"error\",\"efficiency\"])\n",
    "currentIndex = 0\n",
    "for channelL,channelR in zip(roughChannels[:,0],roughChannels[:,1]):\n",
    "    for depth in useDOIs:\n",
    "        \n",
    "        # get all instances at specific channel and DOI\n",
    "        whereDOI = np.where((resultFrame.ChannelIDL == channelL) & (resultFrame.Truth == depth))[0]\n",
    "\n",
    "        data = resultFrame.Truth.to_numpy()[whereDOI] - resultFrame.Predicted.to_numpy()[whereDOI]\n",
    "        \n",
    "        y,x = np.histogram(data,50)\n",
    "        centers = (x[:-1] + x[1:]) / 2\n",
    "        try:\n",
    "            \n",
    "            # construct a reasonable guess for our fit\n",
    "            a = np.where(y == max(y))[0][0]\n",
    "            mu = centers[a]\n",
    "            A = y[a]\n",
    "            std = 0.1\n",
    "            guess = [A,mu,std]\n",
    "            \n",
    "            \n",
    "            # fit to Gaussian to DOI\n",
    "            p,c = curve_fit(gaussian,centers,y)\n",
    "            p = abs(p)\n",
    "                       \n",
    "            FWHM = p[2]*2.355\n",
    "            ERR = np.sqrt(c[2,2])*2.355/np.sqrt(N) # standard error calculation\n",
    "            \n",
    "            # calculating efficiency\n",
    "            inside = np.where((centers <= p[1] + 2.5*p[2]) & (centers >= p[1] - 2.5*p[2]))\n",
    "            outside = np.where((centers >= p[1] + 2.5*p[2]) | (centers <= p[1] - 2.5*p[2]))\n",
    "            yinside = y[inside]\n",
    "            youtside = y[outside]\n",
    "            \n",
    "            efficiency = 1 - np.sum(youtside)/(np.sum(youtside) + np.sum(yinside))\n",
    "            \n",
    "            resolutionFrame.loc[currentIndex] = [channelL,channelR,depth,FWHM,ERR,efficiency]\n",
    "            \n",
    "            currentIndex += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if print_averages == True:\n",
    "    print()\n",
    "    if omit_2_28 == True:\n",
    "        for depth in useDOIs:\n",
    "            res = np.round(resolutionFrame[resolutionFrame.DOI == depth].FWHM.mean(),3)\n",
    "            eff = np.round(resolutionFrame[resolutionFrame.DOI == depth].efficiency.mean(),3)\n",
    "            print(\"Average Resolution at {} mm DOI: {} mm with {}% effciency\".format(depth,res,eff*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
